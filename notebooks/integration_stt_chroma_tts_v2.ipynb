{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1b82ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q python-dotenv langchain-openai langchain-community pydub librosa numpy uuid chromadb langchain-chroma google-cloud-texttospeech"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafe0c17",
   "metadata": {},
   "source": [
    "## ChromaDB class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13478a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import chromadb\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from uuid import uuid4\n",
    "\n",
    "class ChromaManager:\n",
    "    \"\"\"\n",
    "    Utility class for managing ChromaDB Cloud duties\n",
    "    via langchain.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the ChromaDB Cloud Client.\n",
    "\n",
    "        Environment variables:\n",
    "            CHROMA_API_KEY (str): Your ChromaDB Cloud API key.\n",
    "            CHROMA_TENANT (str): Your ChromaDB Cloud tenant ID.\n",
    "            CHROMA_DATABASE (str): Your ChromaDB Cloud database name.\n",
    "        \"\"\"\n",
    "        load_dotenv(find_dotenv())\n",
    "\n",
    "        if not os.getenv(\"CHROMA_API_KEY\") or not os.getenv(\"CHROMA_TENANT\") or not os.getenv(\"CHROMA_DATABASE\"):\n",
    "            raise EnvironmentError(\"ChromaDB environment variables are not set. Please set CHROMA_API_KEY, CHROMA_TENANT, and CHROMA_DATABASE.\")\n",
    "        \n",
    "        self.client = chromadb.CloudClient(\n",
    "            api_key=os.getenv(\"CHROMA_API_KEY\"),\n",
    "            tenant=os.getenv(\"CHROMA_TENANT\"),\n",
    "            database=os.getenv(\"CHROMA_DATABASE\")\n",
    "        )\n",
    "        self.embedding_function = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "    def get_collection(self, collection_name: str) -> Chroma:\n",
    "        \"\"\"\n",
    "        Creates or retrieves and returns a particular ChromaDB collection.\n",
    "        \n",
    "        Args:\n",
    "            collection_name (str): Name of the desired collection to create or retrieve.\n",
    "\n",
    "        Returns:\n",
    "            Chroma: LangChain-compatible Chroma collection.\n",
    "        \"\"\"\n",
    "        if not collection_name:\n",
    "            raise ValueError(\"Collection name must be provided.\")\n",
    "        \n",
    "        return Chroma(\n",
    "            client=self.client,\n",
    "            embedding_function=self.embedding_function,\n",
    "            collection_name=collection_name,\n",
    "        )\n",
    "    \n",
    "    def save_documents(self, docs: list[Document], collection_name: str, tag: str):\n",
    "        \"\"\"\n",
    "        Save documents to a ChromaDB collection.\n",
    "\n",
    "        Args:\n",
    "            docs (list[Document]): Documents to be stored.\n",
    "            collection_name (str): Target collection name.\n",
    "            tag (str): Metadata tag to associate with the documents.\n",
    "        \"\"\"\n",
    "        if not isinstance(docs, list):\n",
    "            raise TypeError(\"Expected a list of Document objects.\")\n",
    "        if not docs:\n",
    "            raise ValueError(\"No documents provided.\")\n",
    "        if not collection_name:\n",
    "            raise ValueError(\"Collection name must be provided.\")\n",
    "        if not tag:\n",
    "            raise ValueError(\"Tag must be provided.\")\n",
    "\n",
    "        documents = []\n",
    "        docIds = []  # To ensure unique IDs\n",
    "\n",
    "        for doc in docs:\n",
    "            unique_id = str(uuid4())\n",
    "\n",
    "            # Re-traitement mais utile pour ajouter des infos dans la BDD\n",
    "            documents.append(\n",
    "                Document(\n",
    "                    page_content=doc.page_content,\n",
    "                    metadata={\n",
    "                        \"tag\": tag,\n",
    "                        \"id\": unique_id\n",
    "                    }\n",
    "                )\n",
    "            )\n",
    "            docIds.append(unique_id)\n",
    "        \n",
    "        collection = self.get_collection(collection_name=collection_name)\n",
    "        collection.add_documents(documents=documents, ids=docIds)\n",
    "\n",
    "    def retrieve_many(self, query: str, collection_name: str, nResults: int = 10) -> list[str]:\n",
    "        \"\"\"\n",
    "        Retrieve multiple documents matching a query from a ChromaDB collection.\n",
    "\n",
    "        Args:\n",
    "            query (str): Query string.\n",
    "            collection_name (str): Name of the collection to search.\n",
    "            nResults (int): Maximum number of results to return.\n",
    "\n",
    "        Returns:\n",
    "            list[str]: List of matching document contents. Max 10 documents.\n",
    "        \"\"\"\n",
    "        if not isinstance(query, str) or not query.strip():\n",
    "            raise ValueError(\"Query must be a non-empty string.\")\n",
    "        if not collection_name:\n",
    "            raise ValueError(\"Collection name must be provided.\")\n",
    "        if not isinstance(nResults, int) or nResults <= 0:\n",
    "            raise ValueError(\"nResults must be a positive integer.\")\n",
    "        if nResults > 10:\n",
    "            raise ValueError(\"nResults cannot exceed 10 for performance reasons.\")\n",
    "        \n",
    "        collection = self.get_collection(collection_name=collection_name)\n",
    "        retriever = collection.as_retriever(\n",
    "            search_type=\"mmr\", \n",
    "            search_kwargs={\"k\": nResults, \"fetch_k\": nResults * 10}\n",
    "        )\n",
    "\n",
    "        documents_found = retriever.invoke(input=query)\n",
    "\n",
    "        results = []\n",
    "\n",
    "        if documents_found:\n",
    "            for doc in documents_found:\n",
    "                if doc.page_content:\n",
    "                    results.append(doc.page_content)\n",
    "\n",
    "        return results\n",
    "    \n",
    "    def retrieve_one(self, query: str, collection_name: str) -> str | None:\n",
    "        \"\"\"\n",
    "        Retrieve a single document matching a query from a ChromaDB collection.\n",
    "\n",
    "        Args:\n",
    "            query (str): Query string.\n",
    "            collection_name (str): Name of the collection to search.\n",
    "\n",
    "        Returns:\n",
    "            str|None: Matching document content, or None if no match found.\n",
    "        \"\"\"\n",
    "        if not isinstance(query, str) or not query.strip():\n",
    "            raise ValueError(\"Query must be a non-empty string.\")\n",
    "        elif not collection_name:\n",
    "            raise ValueError(\"Collection name must be provided.\")\n",
    "\n",
    "        collection = self.get_collection(collection_name=collection_name)\n",
    "        retriever = collection.as_retriever(\n",
    "            search_type=\"mmr\",\n",
    "            search_kwargs={\"k\": 1, \"fetch_k\": 10}\n",
    "        )\n",
    "\n",
    "        documents_found = retriever.invoke(input=query)\n",
    "\n",
    "        if documents_found and isinstance(documents_found, list):\n",
    "            for doc in documents_found:\n",
    "                if doc.page_content:\n",
    "                    return doc.page_content\n",
    "\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4600973",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ma nourriture préférée est la pizza.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db = ChromaManager()\n",
    "\n",
    "db.retrieve_one(\"Quelle est ma nourriture préférée ?\", \"test_collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cae81dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Joachim, forgeron d'ombre au regard d'ambre brûlant, façonne des armes vivantes à partir des peurs des hommes. Il erre entre les ruines d'un monde brisé, traqué par les créatures qu'il a lui-même libérées du néant.\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.retrieve_one(\"Who is Joachim ?\", \"test_collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39f3a537",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ma nourriture préférée est la pizza.',\n",
       " \"J'adore le chocolat !\",\n",
       " \"J'adore également les sushis.\",\n",
       " 'je déteste les huîtres\\n',\n",
       " \"Kaelys, ancien archiviste du royaume d'Aelion, manie les secrets oubliés comme d'autres manie l'acier. Sous ses robes élimées se cache un pacte interdit, scellé avec une entité qu'il nomme simplement la Voix.\\n\"]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.retrieve_many(\"Quelle est ma nourriture préférée ?\", \"test_collection\", nResults=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fa8ada",
   "metadata": {},
   "source": [
    "## Speech-To-Text class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ba18bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders.parsers.audio import OpenAIWhisperParser\n",
    "from langchain_community.document_loaders.generic import GenericLoader\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "class SpeechToTextManager:\n",
    "    \"\"\"\n",
    "    Utility class for transcribing audio files into text using OpenAI Whisper,\n",
    "    via LangChain integration.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lang=\"fr\", model=\"whisper-1\"):\n",
    "        \"\"\"\n",
    "        Initialize the OpenAI Whisper parser with preset parameters for French audio,\n",
    "        deterministic output, and plain text format.\n",
    "\n",
    "        Environment variables:\n",
    "            OPENAI_API_KEY (str): Your OpenAI API key.\n",
    "        \n",
    "        Args:\n",
    "            lang (str): Language to use. Defaults to French (ici c'est Paris !)\n",
    "            model (str): Name of the OpenAI model. Defaults to whisper-1.\n",
    "        \"\"\"\n",
    "        load_dotenv(find_dotenv())\n",
    "\n",
    "        if not os.getenv('OPENAI_API_KEY'):\n",
    "            raise EnvironmentError(\"OPENAI_API_KEY environment variable is not set.\")\n",
    "        elif not lang:\n",
    "            raise ValueError(\"Language must be specified.\")\n",
    "        elif not model:\n",
    "            raise ValueError(\"Model must be specified.\")\n",
    "        \n",
    "        self.parser = OpenAIWhisperParser(\n",
    "            api_key=os.getenv('OPENAI_API_KEY'),\n",
    "            chunk_duration_threshold=0.7,  # Disregard audio chunks shorter than 0.7 seconds\n",
    "            language=lang,  # Specify the language of the audio\n",
    "            response_format=\"text\",\n",
    "            temperature=0.0,  # Deterministic output\n",
    "            model=model\n",
    "        )\n",
    "\n",
    "    def transcribe(self, file_path: str) -> str:\n",
    "        \"\"\"\n",
    "        Transcribe an audio file into a list of LangChain Document objects,\n",
    "        ready for downstream processing like summarization, retrieval, or indexing.\n",
    "        \n",
    "        Args:\n",
    "            file_path (str): Path to the audio file to be transcribed.\n",
    "        \n",
    "        Returns:\n",
    "            str: The transcribed text from the audio file.\n",
    "        \"\"\"\n",
    "\n",
    "        if not os.path.isfile(file_path):\n",
    "            raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n",
    "        elif not file_path.lower().endswith(('.mp3', '.wav', '.flac', '.m4a', '.ogg')):\n",
    "            raise ValueError(\"Unsupported audio file format. Supported formats: .mp3, .wav, .flac, .m4a, .ogg\")\n",
    "        \n",
    "        loader = GenericLoader.from_filesystem(\n",
    "            path=file_path,\n",
    "            parser=self.parser\n",
    "        )\n",
    "\n",
    "        docs = loader.load()\n",
    "        \n",
    "        if not docs:\n",
    "            raise ValueError(\"No documents were loaded from the audio file.\")\n",
    "        elif not isinstance(docs, list):\n",
    "            raise TypeError(\"Expected a list of Document objects from the audio file.\")\n",
    "        \n",
    "        return docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9032346e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcribing part 1!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Bonjour, je m'appelle Joachim Jasmin. Ceci est un enregistrement.\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stt = SpeechToTextManager()\n",
    "transcribed_docs = stt.transcribe(\"../data/audio/test_whisper.m4a\")\n",
    "transcribed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c57a340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcribing part 1!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Bonjour, j'aimerais savoir quelle est la situation géopolitique de Cuba actuellement le 15 juillet à 17h11.\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stt = SpeechToTextManager()\n",
    "transcribed_docs = stt.transcribe(\"../data/audio/test_whisper_clovis.m4a\")\n",
    "transcribed_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c6bcde",
   "metadata": {},
   "source": [
    "## Text-to-Speech class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1da7f204",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.cloud import texttospeech\n",
    "from uuid import uuid4\n",
    "\n",
    "class TextToSpeechManager:\n",
    "    \"\"\"\n",
    "    Utility class for synthesizing speech from text using Google Text-to-Speech,\n",
    "    via LangChain integration.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, credentials_path: str = \"../secrets/google_tts.json\"):\n",
    "        \"\"\"\n",
    "        Initialize the Google Text-to-Speech client with the provided credentials.\n",
    "\n",
    "        Args:\n",
    "            credentials_path (str): Path to the Google Cloud service account JSON file.\n",
    "        \"\"\"\n",
    "        if not os.path.isfile(credentials_path):\n",
    "            raise FileNotFoundError(f\"The credentials file {credentials_path} does not exist.\")\n",
    "        \n",
    "        os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = credentials_path\n",
    "        self.client = texttospeech.TextToSpeechClient()\n",
    "        self.audio_path = None  # Placeholder for the audio file path\n",
    "\n",
    "    def synthesize_speech(self, text: str, output_folder: str, lang_code=\"fr-FR\", voice_name=\"fr-FR-Chirp3-HD-Charon\") -> str:\n",
    "        \"\"\"\n",
    "        Synthesize speech from the provided text using Google Text-to-Speech.\n",
    "\n",
    "        Args:\n",
    "            text (str): The text to be converted to speech.\n",
    "            output_folder (str): The folder path where the synthesized audio will be saved.\n",
    "            lang_code (str): Language code for the voice. Defaults to \"fr-FR\".\n",
    "            voice_name (str): Name of the voice to use. Defaults to \"fr-FR-Chirp3-HD-Charon\".\n",
    "\n",
    "        Returns:\n",
    "            str: The file path where the audio content is saved.\n",
    "        \"\"\"\n",
    "        if not text or not isinstance(text, str):\n",
    "            raise ValueError(\"Text must be a non-empty string.\")\n",
    "        if not output_folder or not isinstance(output_folder, str):\n",
    "            raise ValueError(\"Output folder must be a non-empty string.\")\n",
    "        if not lang_code or not isinstance(lang_code, str):\n",
    "            raise ValueError(\"Language code must be a non-empty string.\")\n",
    "        if not voice_name or not isinstance(voice_name, str):\n",
    "            raise ValueError(\"Voice name must be a non-empty string.\")\n",
    "        \n",
    "        synthesis_input = texttospeech.SynthesisInput(text=text)\n",
    "        voice = texttospeech.VoiceSelectionParams(\n",
    "            language_code=lang_code,\n",
    "            name=voice_name\n",
    "        )\n",
    "        audio_config = texttospeech.AudioConfig(\n",
    "            audio_encoding=texttospeech.AudioEncoding.MP3\n",
    "        )\n",
    "\n",
    "        response = self.client.synthesize_speech(\n",
    "            input=synthesis_input,\n",
    "            voice=voice,\n",
    "            audio_config=audio_config\n",
    "        )\n",
    "\n",
    "        if not response.audio_content:\n",
    "            raise RuntimeError(\"Failed to synthesize speech. No audio content returned.\")\n",
    "\n",
    "        # Save the audio content to the specified output folder\n",
    "        self.audio_path = os.path.join(output_folder, f\"{uuid4()}.mp3\")\n",
    "\n",
    "        with open(self.audio_path, \"wb\") as out:\n",
    "            out.write(response.audio_content)\n",
    "\n",
    "        return self.audio_path\n",
    "    \n",
    "    def get_audio_path(self) -> str:\n",
    "        \"\"\"\n",
    "        Get the path of the last synthesized audio file.\n",
    "\n",
    "        Returns:\n",
    "            str: The path of the last synthesized audio file.\n",
    "        \"\"\"\n",
    "        if not self.audio_path:\n",
    "            raise ValueError(\"No audio has been synthesized yet.\")\n",
    "        \n",
    "        return self.audio_path\n",
    "\n",
    "    def clear_audio_path(self):\n",
    "        \"\"\"\n",
    "        Clear the stored audio file.\n",
    "        \"\"\"\n",
    "        if self.audio_path and os.path.isfile(self.audio_path):\n",
    "            os.remove(self.audio_path)\n",
    "            self.audio_path = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3e8a111",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../tmp/b392975b-7c55-4160-8b92-c48f71245480.mp3'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tts = TextToSpeechManager()\n",
    "output_path = tts.synthesize_speech(\n",
    "    text=\"Hello, this is a test in English.\",\n",
    "    output_folder=\"../tmp\",\n",
    "    lang_code=\"en-US\",\n",
    "    voice_name=\"en-US-Wavenet-D\"\n",
    ")\n",
    "output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3397d2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tts.clear_audio_path()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abae1776",
   "metadata": {},
   "source": [
    "## Recorder class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2e06d901",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import threading\n",
    "import queue\n",
    "from scipy.io.wavfile import write\n",
    "from uuid import uuid4\n",
    "\n",
    "class AudioRecorder:\n",
    "    \"\"\"\n",
    "    Class to handle audio recording from the microphone.\n",
    "    Uses sounddevice for real-time audio input.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_folder=\"../tmp\"):\n",
    "        \"\"\"\n",
    "        Initialize the audio recorder with optional STT manager.\n",
    "\n",
    "        Args:\n",
    "            output_folder (str): Folder where the recorded audio will be saved.\n",
    "        \"\"\"\n",
    "        self.samplerate = 44100\n",
    "        self.stream = None\n",
    "        self.recording = []\n",
    "        self.queue = queue.Queue()\n",
    "        self.output_folder = output_folder\n",
    "        self.audio_path = None\n",
    "\n",
    "    def start(self, _=None):\n",
    "        \"\"\"\n",
    "        Start recording audio from the microphone.\n",
    "\n",
    "        This method initializes the audio stream and starts collecting audio chunks\n",
    "        in a separate thread. The audio data is stored in a queue and appended to\n",
    "        the recording list as it is received.\n",
    "        \"\"\"\n",
    "        if self.audio_path:\n",
    "            self.clear_audio_path()  # Clear any previous audio file\n",
    "            \n",
    "        self.recording = []\n",
    "        self.queue = queue.Queue()\n",
    "\n",
    "        self.stream = sd.InputStream(\n",
    "            samplerate=self.samplerate, \n",
    "            channels=1, \n",
    "            callback=lambda indata, *_: self.queue.put(indata.copy())\n",
    "        )\n",
    "        self.stream.start()\n",
    "\n",
    "        def collect_chunks():\n",
    "            while self.stream and self.stream.active:\n",
    "                try:\n",
    "                    chunk = self.queue.get(timeout=0.1)\n",
    "                    self.recording.append(chunk)\n",
    "                except queue.Empty:\n",
    "                    continue\n",
    "\n",
    "        threading.Thread(target=collect_chunks, daemon=True).start()\n",
    "\n",
    "    def stop(self, _=None):\n",
    "        \"\"\"\n",
    "        Stop the audio recording and save it as a WAV file.\n",
    "\n",
    "        This method stops the audio stream, concatenates the recorded audio chunks,\n",
    "        and saves them to a WAV file in the specified output folder. The file is named\n",
    "        with a unique UUID to avoid conflicts.\n",
    "        \"\"\"\n",
    "        if not self.stream or not self.stream.active:\n",
    "            raise RuntimeError(\"Audio stream is not active. Please start recording first.\")\n",
    "\n",
    "        if self.stream and self.stream.active:\n",
    "            self.stream.stop()\n",
    "            self.stream.close()\n",
    "\n",
    "        audio_data = np.concatenate(self.recording, axis=0)\n",
    "        os.makedirs(self.output_folder, exist_ok=True)\n",
    "        self.audio_path = os.path.join(self.output_folder, f\"{uuid4()}.wav\")\n",
    "        \n",
    "        write(self.audio_path, self.samplerate, audio_data)\n",
    "\n",
    "        return self.audio_path\n",
    "\n",
    "    def get_audio_path(self) -> str | None:\n",
    "        \"\"\"\n",
    "        Return the path to the last recorded audio file.\n",
    "\n",
    "        Returns:\n",
    "            str|None: The path of the last recorded audio file, or None if no audio\n",
    "        \"\"\"\n",
    "        return self.audio_path\n",
    "    \n",
    "    def clear_audio_path(self, _=None):\n",
    "        \"\"\"\n",
    "        Clear the stored audio file.\n",
    "        \"\"\"\n",
    "        if self.audio_path and os.path.isfile(self.audio_path):\n",
    "            os.remove(self.audio_path)\n",
    "            self.audio_path = None\n",
    "        else:\n",
    "            raise ValueError(\"No audio file to clear. Please record audio first.\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437a402b",
   "metadata": {},
   "source": [
    "## AudioRecorderUI class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4fee57c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Audio, clear_output\n",
    "\n",
    "class AudioRecorderUI:\n",
    "    \"\"\"\n",
    "    Simple UI for recording audio using IPython widgets.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, recorder: AudioRecorder):\n",
    "        \"\"\"\n",
    "        Initialize the UI with an AudioRecorder instance.\n",
    "\n",
    "        Args:\n",
    "            recorder (AudioRecorder): The audio recorder instance to use.\n",
    "        \"\"\"\n",
    "        self.recorder = recorder\n",
    "        self.record_button = widgets.Button(description=\"Start Recording\")\n",
    "        self.stop_button = widgets.Button(description=\"Stop Recording\")\n",
    "        self.clear_button = widgets.Button(description=\"Clear Audio\")\n",
    "        \n",
    "        self.record_button.on_click(self.recorder.start)\n",
    "        self.stop_button.on_click(self.recorder.stop)\n",
    "        self.clear_button.on_click(self.recorder.clear_audio_path)\n",
    "\n",
    "        display(self.record_button, self.stop_button, self.clear_button)\n",
    "\n",
    "    def show_audio(self):\n",
    "        \"\"\"\n",
    "        Display the recorded audio file if it exists.\n",
    "        \n",
    "        This method checks if an audio file has been recorded and displays it using\n",
    "        IPython's Audio widget. If no audio file is found, it prints a message.\n",
    "        \"\"\"\n",
    "        audio_path = self.recorder.get_audio_path()\n",
    "        if audio_path and os.path.isfile(audio_path):\n",
    "            clear_output(wait=True)\n",
    "            display(self.record_button, self.stop_button, self.clear_button)\n",
    "            display(Audio(audio_path, autoplay=True))\n",
    "        else:\n",
    "            raise ValueError(\"No audio file recorded yet. Please record audio first.\")\n",
    "\n",
    "    def clear_audio(self):\n",
    "        \"\"\"\n",
    "        Clear the recorded audio file and reset the UI.\n",
    "        \n",
    "        This method removes the audio file from the filesystem and resets the\n",
    "        audio path in the recorder. It also updates the UI to reflect that no\n",
    "        audio is currently available.\n",
    "        \"\"\"\n",
    "        self.recorder.clear_audio_path()\n",
    "        clear_output(wait=True)\n",
    "        display(self.record_button, self.stop_button, self.clear_button)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "26f2f548",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5d3d59fe332420ea8c613daab7a1633",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Start Recording', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0485077b5134369868d6d4db005a935",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Stop Recording', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7ce7be17c3a42c19489f116136d7de0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Clear Audio', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Audio stream is not active. Please start recording first.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 69\u001b[39m, in \u001b[36mAudioRecorder.stop\u001b[39m\u001b[34m(self, _)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     62\u001b[39m \u001b[33;03mStop the audio recording and save it as a WAV file.\u001b[39;00m\n\u001b[32m     63\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     66\u001b[39m \u001b[33;03mwith a unique UUID to avoid conflicts.\u001b[39;00m\n\u001b[32m     67\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stream.active:\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mAudio stream is not active. Please start recording first.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stream \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stream.active:\n\u001b[32m     72\u001b[39m     \u001b[38;5;28mself\u001b[39m.stream.stop()\n",
      "\u001b[31mRuntimeError\u001b[39m: Audio stream is not active. Please start recording first."
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No audio file to clear. Please record audio first.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 100\u001b[39m, in \u001b[36mAudioRecorder.clear_audio_path\u001b[39m\u001b[34m(self, _)\u001b[39m\n\u001b[32m     98\u001b[39m     \u001b[38;5;28mself\u001b[39m.audio_path = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo audio file to clear. Please record audio first.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: No audio file to clear. Please record audio first."
     ]
    }
   ],
   "source": [
    "recorder = AudioRecorder(output_folder=\"../tmp\")\n",
    "recorderUI = AudioRecorderUI(recorder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
